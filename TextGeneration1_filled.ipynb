{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGeneration1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lundez/afCompetenceNLP-ML/blob/master/TextGeneration1_filled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHgMdCRzcMyv",
        "colab_type": "text"
      },
      "source": [
        "## Welcome to ÅF Competence Evening (Machine Learning, NLP)\n",
        "This is part one of a four, or five, part series where we'll go through **text generation**.\n",
        "\n",
        "\n",
        "### Text Generation\n",
        "What to do is self-saying but what we need is some kind of *Language Model*. \n",
        "\n",
        "#### Language Model\n",
        "Language Models as the most simple definition is the probability of a sequence of word as whole. \n",
        "\n",
        "Think of this as `ello`, what does this move towards? `hello` or should we continue and build `mellow` perhaps? The human brain is really good at understanding the context and filling in the blanks. Depending on the \"history\" we have it is easier or harder to guess. The same applies if we use Maximum Likelihood. \n",
        "\n",
        "We can also apply this on a word-level meaning that if we have \"*How are you WORD*\" we would most likely guess \"*WORD*\" to be \"*doing*\".\n",
        "\n",
        "The conclusion is that we need to count N-grams & produce statistics out of these using Markov Chains or something like it. Looking at this we can find the following;  \n",
        "Bigram-model: $p(w) = \\prod_{i=1}^{k+1} p(w_i|w_{i-1})$  \n",
        "\n",
        "To find the probabilities given history we need to find the possibilites given the history,\n",
        "\n",
        "Estimate probabilities: $p(w_i|w_{i-1})=\\frac{c(w_{i-1}w_i)}{c(w_{i-1})}$\n",
        "\n",
        "We can expand this concept to apply to N-grams too. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHnr09HrBC83",
        "colab_type": "text"
      },
      "source": [
        "First we import the needed modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM3-8OBZAazw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import *\n",
        "from random import random\n",
        "import string\n",
        "import numpy as np\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "nlp.max_length=5576562\n",
        "PADDING = \"~\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgmRVERCFTVT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d993024b-5092-4bdd-c046-7920002e58a0"
      },
      "source": [
        "# Can be exchange for other inputs.\n",
        "# e.g. https://github.com/ashwinmj/word-prediction/blob/master/eminem_songs_lyrics.txt\n",
        "!wget http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-25 18:58:48--  http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt [following]\n",
            "--2019-06-25 18:58:48--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4573338 (4.4M) [text/plain]\n",
            "Saving to: ‘shakespeare_input.txt’\n",
            "\n",
            "\rshakespeare_input.t   0%[                    ]       0  --.-KB/s               \rshakespeare_input.t  11%[=>                  ] 520.00K  2.47MB/s               \rshakespeare_input.t 100%[===================>]   4.36M  13.2MB/s    in 0.3s    \n",
            "\n",
            "2019-06-25 18:58:48 (13.2 MB/s) - ‘shakespeare_input.txt’ saved [4573338/4573338]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUDGlObNGbO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "3ae840e0-e84a-480c-de2c-dcbc81c87d76"
      },
      "source": [
        "print(open('shakespeare_input.txt', 'r').read()[:250])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE8baU4Gbnf4",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "We first define training. We want to read a file & have a certain length of \"memory\" ($n$).  \n",
        "We'll start with order (memory) = $2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js48gl4G36Mt",
        "colab_type": "text"
      },
      "source": [
        "### Data\n",
        "\n",
        "Let's start off with the classic - Shakespeare. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrVfnqWk9aZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(counter):\n",
        "    s = float(sum(counter.values()))\n",
        "    return [(c, cnt / s) for c, cnt in counter.items()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SKbMyr13Ywk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_char_lm(fname, order=4):\n",
        "    with open(fname, 'r') as f:\n",
        "        data = f.read()\n",
        "\n",
        "        lm = defaultdict(Counter)\n",
        "        pad = PADDING * order\n",
        "        data = pad + data\n",
        "        for i in range(len(data)-order):\n",
        "            history, char = data[i:i+order], data[i+order]\n",
        "            lm[history][char] += 1\n",
        "\n",
        "        outlm = {hist: normalize(chars) for hist, chars in lm.items()}\n",
        "        return outlm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnIU9haN4COK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm = train_char_lm(\"shakespeare_input.txt\", order=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXPCs2Mb4EzN",
        "colab_type": "text"
      },
      "source": [
        "Let's test the Language Model (lm). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T91PhRB4EDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "88771ce2-867c-42dd-f780-ce86f9f2b14a"
      },
      "source": [
        "lm['ello']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('r', 0.059625212947189095),\n",
              " ('w', 0.817717206132879),\n",
              " ('u', 0.03747870528109029),\n",
              " (',', 0.027257240204429302),\n",
              " (' ', 0.013628620102214651),\n",
              " ('.', 0.0068143100511073255),\n",
              " ('?', 0.0068143100511073255),\n",
              " (':', 0.005110732538330494),\n",
              " ('n', 0.0017035775127768314),\n",
              " (\"'\", 0.017035775127768313),\n",
              " ('!', 0.0068143100511073255)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS9jSo68mkq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35d22652-04fb-428a-af05-71ee8dd8be60"
      },
      "source": [
        "lm['Firs']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('t', 1.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwtMxYmqmm9j",
        "colab_type": "text"
      },
      "source": [
        "What do we learn from this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg52M_l5mo1b",
        "colab_type": "text"
      },
      "source": [
        "### Generating text\n",
        "Now to the fun part. We want to generate text!\n",
        "\n",
        "To generate text we'll generate one letter (character) at a time. We will look at history and the last order of characters, from this we will sample a letter based on the distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aMZbGW3urg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_letter(lm, history, order):\n",
        "        history = history[-order:]\n",
        "        dist = lm[history]\n",
        "        x = random()\n",
        "        for c,v in dist:\n",
        "            x -= v # Done to have some more randomization\n",
        "            if x <= 0: return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFwlbaNnvGZv",
        "colab_type": "text"
      },
      "source": [
        "But generating letters doesn't make a text, we need something that glues this together. We need to generate the text out of the letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8skv8DtvMcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(lm, order, nletters=1000):\n",
        "    history = PADDING * order\n",
        "    out = []\n",
        "    for i in range(nletters):\n",
        "        c = generate_letter(lm, history, order)\n",
        "        history = history[-order:] + c\n",
        "        out.append(c)\n",
        "    return \"\".join(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2B4NmpSvcoA",
        "colab_type": "text"
      },
      "source": [
        "### Order = 2\n",
        "Let's give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J50YpwHgvd9I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "2fa94219-8886-41e0-abcc-a2966e6fe657"
      },
      "source": [
        "lm = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
        "print(generate_text(lm, 2))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Firseelse; I be! theed\n",
            "She welass. BOTHUS:\n",
            "In ene.\n",
            "Wily dilver. WICKLYSS:\n",
            "Stold hou githist this her my a wit\n",
            "mes\n",
            "O, I as aseir poodyies;\n",
            "TH:\n",
            "Awas pate,\n",
            "Andst comee, brot kine, wer's my my on thave at me that briest forn the not took-hou so?\n",
            "\n",
            "Are will inte grach,\n",
            "As, rin ind thateshisice\n",
            "Telis wordied a wils nown thin the inter friusand was andes\n",
            "So my tim hichat mysence!\n",
            "\n",
            "Whe sone.\n",
            "Thich an al give is a mysam, any sirridam? will whand dris\n",
            "Why, lif dive tosence, hisee I'll me\n",
            "is Prat tres?\n",
            "\n",
            "PISTIO:\n",
            "Yief, be aw you wor unhandy, lauld ey. Dew I sand I de nevink of Cithe we sichoulk ingently he oft not astel, 'two prain lackoodde,\n",
            "Unbots bed here mand be pler se shatterd!\n",
            "\n",
            "Bution theakep fou hem dot ine, wereargoor th,\n",
            "KINIUS:\n",
            "YORENA:\n",
            "O, a mill bottleavened th, will my per i' therl of he king apects,\n",
            "Nay make cove wis shat, arentelibet his be the will thall well on me:\n",
            "Well come ceavy scaughty,\n",
            "Toblear lon thoungs:\n",
            "Forth lour sneve, Hell yous gre,\n",
            "Dide, gathy ing if Nay\n",
            "Reatcher usell do\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "warLqEROvicx",
        "colab_type": "text"
      },
      "source": [
        "### Order = 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SAXEzIrvkLO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "outputId": "3cfb04e7-4fe0-4665-d402-79d5ada72d5b"
      },
      "source": [
        "lm = train_char_lm(\"shakespeare_input.txt\", order=4)\n",
        "print(generate_text(lm, 4))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First, with Cassion you all he piness Paris a slaves will advance more shapes, ever.\n",
            "This own\n",
            "But I can her you. I know my child owl wagger-muggest may life,\n",
            "And queen prink this nighter.\n",
            "\n",
            "CLEOPATRA:\n",
            "It much of him: 'tis not give you will your pathy? when may doom.\n",
            "\n",
            "HOLOFERNES:\n",
            "Horator:\n",
            "\n",
            "First thouse my lords are as ever should entertaking's now shoeing so baseness and man in the sounds how to my cheer:\n",
            "Fare thou speak it by on: we shall with young Arthur imporation as I die;\n",
            "But leadieu, good love;\n",
            "No let used her to but till content.\n",
            "\n",
            "CATESBY:\n",
            "'Tis to conscience not hold Sly's ceremonies, ever stone\n",
            "joy it thought that.\n",
            "\n",
            "JULIA:\n",
            "The no further of grief,\n",
            "Or three forthy Anne,\n",
            "And your France\n",
            "To the guide,\n",
            "Being you shallow stardinals, where hands,\n",
            "Buy thee yield;\n",
            "My speaks.\n",
            "\n",
            "EARL OF DOUGLAS:\n",
            "Assemblest thout to Maste an is no comes.\n",
            "\n",
            "PRINCENTIO:\n",
            "O, full fly tongue-tied lose part of the till haps\n",
            "accusation of Mars, I may bending to our he beauty's\n",
            "commandmen, by night, sweet so.\n",
            "\n",
            "SPEED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqWaMM57voTw",
        "colab_type": "text"
      },
      "source": [
        "### Order = 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgGHl6kQvp-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "ffa7e5cb-52cb-46ae-c5d8-0cc1ee5d8450"
      },
      "source": [
        "lm = train_char_lm(\"shakespeare_input.txt\", order=7)\n",
        "print(generate_text(lm, 7))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Tear-falling this lands and so well, 'tis better to the Garter.\n",
            "\n",
            "MONTANO:\n",
            "Nay, gentlemen have the news: he fishermen, that I would\n",
            "the guise,\n",
            "Is all your mother sits:\n",
            "O, by the looks upon the length?\n",
            "\n",
            "OSRIC:\n",
            "I know my\n",
            "place and leave to live in your\n",
            "colour, and town:\n",
            "Go you, that vast tennis-balls, nor Nature, ransom of my place of slaughter,\n",
            "Giving art.\n",
            "You took measure.\n",
            "\n",
            "SUFFOLK:\n",
            "Nay, mother's boy.\n",
            "\n",
            "SUFFOLK:\n",
            "Gelidus timorous ass, your heels of Phoebus in heart.\n",
            "\n",
            "POLIXENES:\n",
            "I do, sir, receive you see any harm?\n",
            "\n",
            "OLIVER:\n",
            "Far true,\n",
            "The head of ladies, and\n",
            "most coldest and into my shame\n",
            "To stop your idleness in blows that pass for life in the commonalty: there, thou diest on my absence, even shortens not that my residing heard, sick'd at, the name of all your honour\n",
            "Than thee again, all before his sound thee! and so set his nets; but taking cause into truth and heathen\n",
            "philosopher\n",
            "Than to another\n",
            "Against a change a mind to life!\n",
            "If my officers\n",
            "Of the perilous gash, a very a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxSiWAdnvr3G",
        "colab_type": "text"
      },
      "source": [
        "### Order = 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18zzb6scvtdf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "2b001036-f520-40e3-b80e-e43b09dd8a8d"
      },
      "source": [
        "lm = train_char_lm(\"shakespeare_input.txt\", order=10)\n",
        "print(generate_text(lm, 10))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Ay, and said 'Ay.'\n",
            "\n",
            "LADY CAPULET:\n",
            "\n",
            "JULIET:\n",
            "O shut the spring to them.\n",
            "\n",
            "POMPEY:\n",
            "You will lose for me one doubt by death\n",
            "Revives two greater a great\n",
            "deal in evil: he excels his brother Hector?\n",
            "\n",
            "PATROCLUS:\n",
            "The fires i' the world increases, and kindly power\n",
            "Than let him smell\n",
            "His way to Dover?\n",
            "\n",
            "EDGAR:\n",
            "Shall Caesar seem ambitious Sylla, overgorged\n",
            "With good advice.\n",
            "\n",
            "CYMBELINE:\n",
            "And youthful men,\n",
            "Who give the dozen white louses do become a Christian blood\n",
            "And hold their malt with water and doth affect\n",
            "A saucy roughness, and be hanged, sir, if now I be one.\n",
            "\n",
            "LYSANDER:\n",
            "A good persuasion? Do not tempt a minister to a mind diseased,\n",
            "Pluck from them\n",
            "And fetch shrill echoes from their exile:\n",
            "They are all welcome you.\n",
            "\n",
            "Officer:\n",
            "I do; and will do the service to your eyes' anguish.\n",
            "\n",
            "GLOUCESTER:\n",
            "It is his.\n",
            "\n",
            "EDMUND:\n",
            "Sound!\n",
            "\n",
            "Herald:\n",
            "I summon your great designs.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Poor painted gloss discovered:\n",
            "That done, trudge with it then; it lies in your\n",
            "head: what malice was a great argu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrbxzBRovvT9",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion of step 1\n",
        "We find that at order = 4 we get reasonable results that only get better with the higher order.\n",
        "\n",
        "We can find that this generation does not support out-of-vocabulary, and can only generate history. This is \"so-so\".  \n",
        "With the famous LSTM (RNN) char2char we can generate new texts & with its memory it can remember to open and end paranthesis, to know that an birthdate is connected with a location often and so on. LSTM can remember for a looong time, as we will see in the (probably) next workshop.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99kV9gXJzBO9",
        "colab_type": "text"
      },
      "source": [
        "### Word by Word Text Generation\n",
        "Let's spin this around to instead generate words, will it work?  \n",
        "Of course it will when we're driving the vehicle!  \n",
        "\n",
        "we want learn a function $P(w|h)$. Here, $w$ is a word, $h$ is a n-word history, and $P(w|h)$ stands for how likely is it to see $w$ after we've seen $h$. See earlier explanation for Bigram-model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o40OiFT8OFNh",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocessing (over & over again!)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk9y5OMnQCuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "  doc = nlp(text,  disable=['parser', 'tagger', 'ner'])\n",
        "  return [str(token) for token in doc]\n",
        "\n",
        "def preprocess(text):\n",
        "  return str(text).lower()\n",
        "  \n",
        "def pandas_preprocess(dataframe):\n",
        "  dataframe = dataframe.applymap(preprocess)\n",
        "  return dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTGWS2xRfdI5",
        "colab_type": "text"
      },
      "source": [
        "#### Training word level generation language model\n",
        "First, just as with character-level, we need to train the model (count words that is).  \n",
        "The same techniques is applied as with character level generation with the difference that we now count words. And we require more data for a good generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVvzdXpa-gtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_word(lm, history, order):\n",
        "    history = history[-order:]\n",
        "    history_key = ' '.join(history)\n",
        "    dist = lm[history_key]\n",
        "    x = random()\n",
        "    for c, v in dist:\n",
        "        x = x - v\n",
        "        if x <= 0: return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtIDmunZ-g8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text_word(lm, order, nletters=25):\n",
        "    history = [PADDING] * order\n",
        "    out = []\n",
        "    for i in range(nletters):\n",
        "        c = generate_word(lm, history, order)\n",
        "        history = history[-order:] + [c]\n",
        "        out.append(c)\n",
        "    return ' '.join(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVrtVRr2e99E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_word_lm(fname, order=2):\n",
        "    with open(fname, 'r') as f:\n",
        "        data = f.read()\n",
        "        words = tokenize(data)\n",
        "        lm = defaultdict(Counter)\n",
        "        pad = [PADDING] * order\n",
        "        data = pad + words\n",
        "        for i in range(len(data)-order):\n",
        "            history, word = data[i:i+order], data[i+order]\n",
        "            lm[' '.join(history)][word] += 1\n",
        "\n",
        "        outlm = {hist: normalize(words) for hist, words in lm.items()}\n",
        "        return outlm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsD2ikpQAHes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "order = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTciO9O0gDA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm = train_word_lm(\"shakespeare_input.txt\", order)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqZ9IjZeACI_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "ce372001-ce00-4999-ccc5-541b95e99d9d"
      },
      "source": [
        "print(generate_text_word(lm, order))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen : \n",
            " The king your father is disposed to sleep . \n",
            "\n",
            " ANTONIO : \n",
            "\n",
            " SEBASTIAN : \n",
            " If you please , \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaY2x9BRuXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Difference to Markov Chain - https://stackoverflow.com/a/24419604\n",
        "# https://blog.dataiku.com/2016/10/08/machine-learning-markov-chains-generate-clinton-trump-quotes\n",
        "# With c2c add <sos> & <eos>, perhaps to word too."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ichgXE_CCWD3",
        "colab_type": "text"
      },
      "source": [
        "## NLTK\n",
        "NLTK is one of the important libraries for someone who works with text. It contains a lot of tooling that can simplify our lives, so let's try to reimplement this using NLTK & their corpuses.\n",
        "\n",
        "Project Gutenberg is a famous corpus containing about 25 000 e-books, including Shakespeare, Jane Austen etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-PIY_5JChtg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "8dd22f62-51f7-4012-de46-f9f9402d6fdc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk import FreqDist"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0h0AHlhCvAK",
        "colab_type": "text"
      },
      "source": [
        "When using NLTK-corpuses that are already processed and beautiful we get some bonuses, we can extract sentences, ord or raw.  \n",
        "When taking the sentences it's already tokenized & ready for use which is pretty damn awesome."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjtgARR3DZf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cc0662b0-3bfa-4d1e-d091-5bf73c386042"
      },
      "source": [
        "print(gutenberg.sents()[:10])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ['CHAPTER', 'I'], ['Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.'], ['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.'], ['Her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', ';', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', ',', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.'], ['Sixteen', 'years', 'had', 'Miss', 'Taylor', 'been', 'in', 'Mr', '.', 'Woodhouse', \"'\", 's', 'family', ',', 'less', 'as', 'a', 'governess', 'than', 'a', 'friend', ',', 'very', 'fond', 'of', 'both', 'daughters', ',', 'but', 'particularly', 'of', 'Emma', '.'], ['Between', '_them_', 'it', 'was', 'more', 'the', 'intimacy', 'of', 'sisters', '.'], ['Even', 'before', 'Miss', 'Taylor', 'had', 'ceased', 'to', 'hold', 'the', 'nominal', 'office', 'of', 'governess', ',', 'the', 'mildness', 'of', 'her', 'temper', 'had', 'hardly', 'allowed', 'her', 'to', 'impose', 'any', 'restraint', ';', 'and', 'the', 'shadow', 'of', 'authority', 'being', 'now', 'long', 'passed', 'away', ',', 'they', 'had', 'been', 'living', 'together', 'as', 'friend', 'and', 'friend', 'very', 'mutually', 'attached', ',', 'and', 'Emma', 'doing', 'just', 'what', 'she', 'liked', ';', 'highly', 'esteeming', 'Miss', 'Taylor', \"'\", 's', 'judgment', ',', 'but', 'directed', 'chiefly', 'by', 'her', 'own', '.'], ['The', 'real', 'evils', ',', 'indeed', ',', 'of', 'Emma', \"'\", 's', 'situation', 'were', 'the', 'power', 'of', 'having', 'rather', 'too', 'much', 'her', 'own', 'way', ',', 'and', 'a', 'disposition', 'to', 'think', 'a', 'little', 'too', 'well', 'of', 'herself', ';', 'these', 'were', 'the', 'disadvantages', 'which', 'threatened', 'alloy', 'to', 'her', 'many', 'enjoyments', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiSgQSrRDhUc",
        "colab_type": "text"
      },
      "source": [
        "As our text is tokenized, let's start of by using what Gutenberg givs us. We can apply `lowercase` later into the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af6YirvMCtWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_word_lm_nltk(order=3):\n",
        "  gut_ngrams = ( ngram for sent in gutenberg.sents() for ngram in ngrams(sent, order, pad_left = True, pad_right = True, right_pad_symbol='EOS', left_pad_symbol=\"BOS\"))\n",
        "  ngram_prob = defaultdict(Counter)\n",
        "  for ngram in gut_ngrams:\n",
        "      ngram_prob[ngram[0] + ngram[1]][ngram[2]] += 1\n",
        "      # ngram_prob[ngram[0]][ngram[2]] += 1 <-- BackOff\n",
        "  outlm = {hist: normalize(chars) for hist, chars in ngram_prob.items()}\n",
        "  return outlm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn1YutSLzAi7",
        "colab_type": "text"
      },
      "source": [
        "### Improvements\n",
        "Okey, this is all good. We can now generate text and we can easily swap the data that we use (just a `.txt` file).  \n",
        "The first improvemet we can do is called _smoothing_. \n",
        "\n",
        "#### Smoothing\n",
        "Smoothing does just what the name suggests, we smooth data. In other words, we allow OOV (out-of-vocabulary) words to be used. This is incredibly important and can help us generate much better text.  \n",
        "\n",
        "**Laplacian Smoothing**  \n",
        "Simplest approach, very naïve. There's two ways, either _add-one smoothing_ or _add-k smoothing_.\n",
        "\n",
        "\n",
        "**Katz-Backoff**  \n",
        "Longer N-grams are better, but if it doesn't exist back off to a shorter one.\n",
        "\n",
        "**Interpolation Smoothing**  \n",
        "Use multiple N in N-grams to get total prob.\n",
        "\n",
        "**Kneser-Key Smoothing**  \n",
        "Most popular one, but hard to implement correctly.  \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*pMttoEXAH_GS9d6AtkhF2g.png)  \n",
        "Very good explanation through a [blog](https://medium.com/@seccon/a-simple-numerical-example-for-kneser-ney-smoothing-nlp-4600addf38b8)\n",
        "\n",
        "#### Smoothing by UNK\n",
        "Another approach could be to smooth the most uncommon words by UNK. This could for example be names if they're rare. In that case we would have names more commonly, and as such perhaps generate \"*UNK was a man of honor*\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj_7ovp2AzBi",
        "colab_type": "text"
      },
      "source": [
        "## Smooth by Backoff & UNK\n",
        "Let's implement smoothing using Backoff & UNK-token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8mAu4jfI9Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BackOff = if < X options for an bigram, choose the unigram prob\n",
        "# UNK = find the least common words, replace them by UNK and redo."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttCYMUBD6mOC",
        "colab_type": "text"
      },
      "source": [
        "### Tips on fun to do at home till next time\n",
        "Markovify: https://github.com/jsvine/markovify\n",
        "\n",
        "This is basically what we've done.\n",
        "\n",
        "### Harder improvements\n",
        "Create a Hidden Markov Model that also makes use of the POS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDUjwCTRBCIP",
        "colab_type": "text"
      },
      "source": [
        "## What's in store for future sessions?\n",
        "\n",
        "\n",
        "*   Neural Networks (will improve result)\n",
        "*   State-of-the-Art Neural Network using GPT-2 & transfer learning\n",
        "*  Generate something really fun (Trump tweets, Rap songs or whatever we decide)\n",
        "*  Deploying a model\n",
        "*  (If people want too; text generation by selecting texts via Word Embedding & such. Example: Zac_the_second_bot)  \n",
        "\n",
        "\n",
        "\n",
        "If all agree & don't have something else they'd prefer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i5I89AgLZXt",
        "colab_type": "code",
        "outputId": "22fc7611-d2bd-4e0f-e236-80220af59664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "gut_ngrams = ( ngram for sent in gutenberg.sents() for ngram in ngrams(sent, 3, pad_left = True, pad_right = True, right_pad_symbol='EOS', left_pad_symbol=\"BOS\"))\n",
        "#print(list(gut_ngrams)[:5])\n",
        "freq_dist = nltk.FreqDist(gut_ngrams)\n",
        "print(freq_dist.keys())\n",
        "kneser_ney = nltk.KneserNeyProbDist(freq_dist)\n",
        "\n",
        "prob_sum = 0\n",
        "for i in kneser_ney.samples():\n",
        "    if i[0] == \"Who\" and i[1] == \"are\":\n",
        "        prob_sum += kneser_ney.prob(i)\n",
        "        print(\"{0}:{1}\".format(i, kneser_ney.prob(i)))\n",
        "print(prob_sum)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('Who', 'are', 'those'):0.03205128205128205\n",
            "('Who', 'are', 'these'):0.057692307692307696\n",
            "('Who', 'are', 'ye'):0.03205128205128205\n",
            "('Who', 'are', 'they'):0.1346153846153846\n",
            "('Who', 'are', 'Israelites'):0.00641025641025641\n",
            "('Who', 'are', 'kept'):0.00641025641025641\n",
            "('Who', 'are', 'you'):0.4166666666666667\n",
            "('Who', 'are', 'YOU'):0.03205128205128205\n",
            "('Who', 'are', 'we'):0.00641025641025641\n",
            "('Who', 'are', 'the'):0.08333333333333333\n",
            "0.8076923076923077\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}